\#TODO introduction, cost function, gradient descent

**The Linear Equation**

For a single input, the linear regression model can be represented as:

$Y = wX + b$

Where:

$Y$ is the predicted value.

$X$ is the independent variable.

$w$ is the weight (or coefficient).

$b$ is the bias (or intercept).

**Learning in Linear Regression**

The objective in linear regression is to find the best line that fits our data. But how do we define "best"?

We use a cost function, often the Mean Squared Error (MSE), to measure the difference between our line's predictions and the actual values. Our goal is to minimize this error.

Using optimization techniques like Gradient Descent, we iteratively adjust the weights and bias to minimize the error.
 
 \#TODO change the structure 
 \# TODO add images 
 \# TODO Math & implementation  